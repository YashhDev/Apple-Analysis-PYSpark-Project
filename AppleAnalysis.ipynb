{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ed1af1-fb9b-48cc-90b7-ff36474d321c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60495280-2ba4-45fe-a1b8-2afb42ab9bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5910bcae-617c-4fc9-8938-50800fb67f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Loader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc69904-7b61-4a2a-8104-5827e0f51900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FirstWorkFlow:\n",
    "    \"\"\"\n",
    "    ETL pipeline to generate the data for customer who have \n",
    "    bought airpods just after buying iphone\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def runner(self):\n",
    "        #Step-1 Extract all required data from different soruce\n",
    "        inputDFs = AirpodsAfterIphoneExtractor().extractor()\n",
    "\n",
    "        #Step-2 Implement the transformer logic\n",
    "        # Customer who have brought airpods after iphone\n",
    "        AirpodsAfterIphoneTransformerDF = AirpodsAfterIphoneTransformer().transform(inputDFs)\n",
    "\n",
    "        #Step 3: Load all the required data to different sink\n",
    "        AirPodsAfterIphoneLoader(AirpodsAfterIphoneTransformerDF).sink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5dc1f3-3cc3-4ee1-aca7-1935edb68fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SecondWorkFlow:\n",
    "    \"\"\"\n",
    "    ETL pipeline to generate the data for customer who have \n",
    "    bought only airpods and iphone\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def runner(self):\n",
    "        #Step-1 Extract all required data from different soruce\n",
    "        inputDFs = AirpodsAfterIphoneExtractor().extractor()\n",
    "\n",
    "        #Step-2 Implement the transformer logic\n",
    "        # Customer who have brought airpods and  iphone\n",
    "        onlyAirpodsAndIphoneDF = OnlyAirpodsAndIphoneTransformer().transform(inputDFs)\n",
    "\n",
    "        #Step 3: Load all the required data to different sink\n",
    "        OnlyAirpodsAndIPhoneLoader(onlyAirpodsAndIphoneDF).sink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13fad778-6128-471d-8732-8612af9fcf15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nCustomer Delta Table\n+-----------+-------------+----------+--------+\n|customer_id|customer_name| join_date|location|\n+-----------+-------------+----------+--------+\n|        105|          Eva|2022-01-01|    Ohio|\n|        106|        Frank|2022-02-01|  Nevada|\n|        107|        Grace|2022-03-01|Colorado|\n|        108|        Henry|2022-04-01|    Utah|\n+-----------+-------------+----------+--------+\n\ntransactionInputDF in transform\n+-----------+--------------------+\n|customer_id|            products|\n+-----------+--------------------+\n|        107|   [AirPods, iPhone]|\n|        108|   [AirPods, iPhone]|\n|        106|[AirPods, iPhone,...|\n|        105|[AirPods, iPhone,...|\n+-----------+--------------------+\n\nOnly Airpods and iPhone\n+-----------+-----------------+\n|customer_id|         products|\n+-----------+-----------------+\n|        107|[AirPods, iPhone]|\n|        108|[AirPods, iPhone]|\n+-----------+-----------------+\n\n+-----------+-------------+----------+--------+\n|customer_id|customer_name| join_date|location|\n+-----------+-------------+----------+--------+\n|        105|          Eva|2022-01-01|    Ohio|\n|        106|        Frank|2022-02-01|  Nevada|\n|        107|        Grace|2022-03-01|Colorado|\n|        108|        Henry|2022-04-01|    Utah|\n+-----------+-------------+----------+--------+\n\nJOINED DF\n+-----------+-------------+----------+--------+-----------------+\n|customer_id|customer_name| join_date|location|         products|\n+-----------+-------------+----------+--------+-----------------+\n|        107|        Grace|2022-03-01|Colorado|[AirPods, iPhone]|\n|        108|        Henry|2022-04-01|    Utah|[AirPods, iPhone]|\n+-----------+-------------+----------+--------+-----------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1726746523661432>:12\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m SecondWorkFlow()\u001B[38;5;241m.\u001B[39mrunner()\n",
       "\u001B[1;32m     10\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecondWorkFlow\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 12\u001B[0m workFolrunner \u001B[38;5;241m=\u001B[39m WorkFlowRunner(name)\u001B[38;5;241m.\u001B[39mrunner()\n",
       "\n",
       "File \u001B[0;32m<command-1726746523661432>:8\u001B[0m, in \u001B[0;36mWorkFlowRunner.runner\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m FirstWorkFlow()\u001B[38;5;241m.\u001B[39mrunner()\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecondWorkFlow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSecondWorkFlow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-3963767845158894>:17\u001B[0m, in \u001B[0;36mSecondWorkFlow.runner\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m #Step-2 Implement the transformer logic\n",
       "\u001B[1;32m     13\u001B[0m # Customer who have brought airpods after iphone\n",
       "\u001B[1;32m     14\u001B[0m secondTransformDF = OnlyAirpodsAndIphoneTransformer().transform(inputDFs)\n",
       "\u001B[1;32m     16\u001B[0m #Step 3: Load all the required data to different sink\n",
       "\u001B[0;32m---> 17\u001B[0m # OnlyAirpodsAndIphoneTransformer(secondTransformDF).sink()\n",
       "\n",
       "File \u001B[0;32m<command-1726746523661577>:25\u001B[0m, in \u001B[0;36msink\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m     def sink(self):\n",
       "\u001B[1;32m     12\u001B[0m         get_sink_source(\n",
       "\u001B[1;32m     13\u001B[0m             sink_type = \"dbfs\",\n",
       "\u001B[1;32m     14\u001B[0m             df = self.transformedDF, \n",
       "\u001B[1;32m     15\u001B[0m             path = \"dbfs:/FileStore/tables/\", \n",
       "\u001B[1;32m     16\u001B[0m             method = \"overwrite\"\n",
       "\u001B[1;32m     17\u001B[0m         ).load_data_frame()\n",
       "\u001B[1;32m     19\u001B[0m # class OnlyAirpodsAndIPhoneLoader(AbstractLoader):\n",
       "\u001B[1;32m     20\u001B[0m \n",
       "\u001B[1;32m     21\u001B[0m #     def sink(self):\n",
       "\u001B[1;32m     22\u001B[0m #         params = {\n",
       "\u001B[1;32m     23\u001B[0m #             \"partitionByColumns\": [\"location\"]\n",
       "\u001B[1;32m     24\u001B[0m #         }\n",
       "\u001B[0;32m---> 25\u001B[0m #         get_sink_source(\n",
       "\u001B[1;32m     26\u001B[0m #             sink_type = \"dbfs_with_partition\",\n",
       "\u001B[1;32m     27\u001B[0m #             df = self.transformedDF, \n",
       "\u001B[1;32m     28\u001B[0m #             path = \"dbfs:/FileStore/tables/apple_analysis\", \n",
       "\u001B[1;32m     29\u001B[0m #             method = \"overwrite\",\n",
       "\u001B[1;32m     30\u001B[0m #             params = params\n",
       "\u001B[1;32m     31\u001B[0m #         ).load_data_frame()\n",
       "\u001B[1;32m     32\u001B[0m \n",
       "\u001B[1;32m     33\u001B[0m #         get_sink_source(\n",
       "\u001B[1;32m     34\u001B[0m #             sink_type = \"delta\",\n",
       "\u001B[1;32m     35\u001B[0m #             df = self.transformedDF, \n",
       "\u001B[1;32m     36\u001B[0m #             path = \"default.onlyAirPodsAndIphone\", \n",
       "\u001B[1;32m     37\u001B[0m #             method = \"overwrite\",\n",
       "\u001B[1;32m     38\u001B[0m #         ).load_data_frame()           \n",
       "\n",
       "File \u001B[0;32m<command-1726746523661503>:31\u001B[0m, in \u001B[0;36mLoadToDBFSWithPartition.load_data_frame\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data_frame\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
       "\u001B[1;32m     30\u001B[0m     partitionByColumns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartitionByColumns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpartitionByColumns\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Partition columns do not match the partition columns of the table.\n",
       "Given: [`location`]\n",
       "Table: []\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1726746523661432>:12\u001B[0m\n\u001B[1;32m      8\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m SecondWorkFlow()\u001B[38;5;241m.\u001B[39mrunner()\n\u001B[1;32m     10\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecondWorkFlow\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m workFolrunner \u001B[38;5;241m=\u001B[39m WorkFlowRunner(name)\u001B[38;5;241m.\u001B[39mrunner()\n\nFile \u001B[0;32m<command-1726746523661432>:8\u001B[0m, in \u001B[0;36mWorkFlowRunner.runner\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m FirstWorkFlow()\u001B[38;5;241m.\u001B[39mrunner()\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecondWorkFlow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSecondWorkFlow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-3963767845158894>:17\u001B[0m, in \u001B[0;36mSecondWorkFlow.runner\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     12\u001B[0m #Step-2 Implement the transformer logic\n\u001B[1;32m     13\u001B[0m # Customer who have brought airpods after iphone\n\u001B[1;32m     14\u001B[0m secondTransformDF = OnlyAirpodsAndIphoneTransformer().transform(inputDFs)\n\u001B[1;32m     16\u001B[0m #Step 3: Load all the required data to different sink\n\u001B[0;32m---> 17\u001B[0m # OnlyAirpodsAndIphoneTransformer(secondTransformDF).sink()\n\nFile \u001B[0;32m<command-1726746523661577>:25\u001B[0m, in \u001B[0;36msink\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     11\u001B[0m     def sink(self):\n\u001B[1;32m     12\u001B[0m         get_sink_source(\n\u001B[1;32m     13\u001B[0m             sink_type = \"dbfs\",\n\u001B[1;32m     14\u001B[0m             df = self.transformedDF, \n\u001B[1;32m     15\u001B[0m             path = \"dbfs:/FileStore/tables/\", \n\u001B[1;32m     16\u001B[0m             method = \"overwrite\"\n\u001B[1;32m     17\u001B[0m         ).load_data_frame()\n\u001B[1;32m     19\u001B[0m # class OnlyAirpodsAndIPhoneLoader(AbstractLoader):\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m #     def sink(self):\n\u001B[1;32m     22\u001B[0m #         params = {\n\u001B[1;32m     23\u001B[0m #             \"partitionByColumns\": [\"location\"]\n\u001B[1;32m     24\u001B[0m #         }\n\u001B[0;32m---> 25\u001B[0m #         get_sink_source(\n\u001B[1;32m     26\u001B[0m #             sink_type = \"dbfs_with_partition\",\n\u001B[1;32m     27\u001B[0m #             df = self.transformedDF, \n\u001B[1;32m     28\u001B[0m #             path = \"dbfs:/FileStore/tables/apple_analysis\", \n\u001B[1;32m     29\u001B[0m #             method = \"overwrite\",\n\u001B[1;32m     30\u001B[0m #             params = params\n\u001B[1;32m     31\u001B[0m #         ).load_data_frame()\n\u001B[1;32m     32\u001B[0m \n\u001B[1;32m     33\u001B[0m #         get_sink_source(\n\u001B[1;32m     34\u001B[0m #             sink_type = \"delta\",\n\u001B[1;32m     35\u001B[0m #             df = self.transformedDF, \n\u001B[1;32m     36\u001B[0m #             path = \"default.onlyAirPodsAndIphone\", \n\u001B[1;32m     37\u001B[0m #             method = \"overwrite\",\n\u001B[1;32m     38\u001B[0m #         ).load_data_frame()           \n\nFile \u001B[0;32m<command-1726746523661503>:31\u001B[0m, in \u001B[0;36mLoadToDBFSWithPartition.load_data_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data_frame\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     30\u001B[0m     partitionByColumns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartitionByColumns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpartitionByColumns\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Partition columns do not match the partition columns of the table.\nGiven: [`location`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Partition columns do not match the partition columns of the table.\nGiven: [`location`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class WorkFlowRunner:\n",
    "    def __init__(self,name):\n",
    "        self.name = name;\n",
    "    def runner(self):\n",
    "        if self.name == \"firstWorkFlow\":\n",
    "            return FirstWorkFlow().runner()\n",
    "        elif self.name == \"secondWorkFlow\":\n",
    "            return SecondWorkFlow().runner()\n",
    "\n",
    "name = \"secondWorkFlow\"\n",
    "\n",
    "workFolrunner = WorkFlowRunner(name).runner()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AppleAnalysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
